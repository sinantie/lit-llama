# destination_path: data/opengpt-single_shot-qa-no-special-toks-preamble
destination_path: data/debug
tokenizer_path: checkpoints/lit-llama/tokenizer.model
test_split_size: 2000
max_seq_length: 256
seed: 42
mask_inputs: true
partitions_to_include:
#- single_shot-qa
#- medical_tasks-qa
- conversational-qa
split_conversations_to_examples: false
# tokens that are in the original raw datasets
special_tokens_input:
  user: "<|user|>" # For chat like interactions we want to have a <user> and <ai> token
  ai: "<|ai|>" # See above
  eos: "<|eos|>" # End of stream (one question, or one answer, or one message)
  eod: "<|eod|>" # End of document, or conversation - in other words the text that comes after this token is not related to the text before it
  pad: "<|pad|>" # Padding 

# tokens we would like to appear in the results dataset
special_tokens_output:
  user: "Human:" # For chat like interactions we want to have a <user> and <ai> token
  ai: "AI:" # See above
  eos: "###" # End of stream (one question, or one answer, or one message)
  eod: "" # End of document, or conversation - in other words the text that comes after this token is not related to the text before it
  pad: "<|pad|>" # Padding 